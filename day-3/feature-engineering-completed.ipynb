{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Cleaning & Wrangling with Pandas\n",
                "\n",
                "**Dataset: Malaysia Customer Transactions 2025**\n",
                "\n",
                "Real-world data is messy! This comprehensive guide covers essential Pandas techniques for:\n",
                "1. **Data Quality Assessment**: Finding and understanding data issues\n",
                "2. **Missing Data**: Detection and handling strategies\n",
                "3. **Duplicates**: Finding and removing redundant data\n",
                "4. **Data Validation**: Ensuring data integrity\n",
                "5. **Data Transformation**: Reshaping and creating new features\n",
                "6. **String Operations**: Cleaning text data\n",
                "7. **Combining Data**: Merging and concatenating datasets\n",
                "8. **Grouping & Aggregation**: Summarizing data effectively"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.precision', 2)\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Loading and Initial Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the Malaysia Customer Transactions dataset\n",
                "df = pd.read_csv(\"dummy_malaysia_customer_transactions_2025.csv\")\n",
                "\n",
                "print(\"Dataset Shape:\", df.shape)\n",
                "print(\"\\nColumn Names:\")\n",
                "print(df.columns.tolist())\n",
                "print(\"\\nData Types:\")\n",
                "print(df.dtypes)\n",
                "print(\"\\nMemory Usage:\")\n",
                "print(df.memory_usage(deep=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First look at the data\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get statistical summary\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# More detailed info\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Data Quality Assessment\n",
                "\n",
                "### 2.1 Checking for Missing Values\n",
                "\n",
                "This dataset **already has real missing values** in `name`, `region`, `number_of_purchases`, and `total_transaction_amount`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check missing values in the real dataset\n",
                "df_dirty = df.copy()\n",
                "\n",
                "# Check missing values\n",
                "print(\"Missing Values Count:\")\n",
                "print(df_dirty.isnull().sum())\n",
                "print(\"\\nMissing Values Percentage:\")\n",
                "print((df_dirty.isnull().sum() / len(df_dirty) * 100).round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing data\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Heatmap of missing values\n",
                "missing_matrix = df_dirty.isnull().astype(int)\n",
                "axes[0].imshow(missing_matrix.T, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
                "axes[0].set_yticks(range(len(df_dirty.columns)))\n",
                "axes[0].set_yticklabels(df_dirty.columns)\n",
                "axes[0].set_xlabel('Row Index')\n",
                "axes[0].set_title('Missing Data Pattern (Red = Missing)')\n",
                "\n",
                "# Bar chart of missing counts\n",
                "missing_counts = df_dirty.isnull().sum()\n",
                "missing_counts[missing_counts > 0].plot(kind='barh', ax=axes[1], color='coral')\n",
                "axes[1].set_xlabel('Count of Missing Values')\n",
                "axes[1].set_title('Missing Values by Column')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Identifying Complete Cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rows with any missing value\n",
                "rows_with_missing = df_dirty[df_dirty.isnull().any(axis=1)]\n",
                "print(f\"Rows with missing data: {len(rows_with_missing)}\")\n",
                "print(f\"Complete rows: {len(df_dirty) - len(rows_with_missing)}\")\n",
                "\n",
                "# Show some examples\n",
                "print(\"\\nExamples of rows with missing data:\")\n",
                "rows_with_missing.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Handling Missing Data\n",
                "\n",
                "### 3.1 Dropping Missing Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop rows with ANY missing values\n",
                "df_dropped_any = df_dirty.dropna()\n",
                "print(f\"Original shape: {df_dirty.shape}\")\n",
                "print(f\"After dropna(): {df_dropped_any.shape}\")\n",
                "\n",
                "# Drop rows with ALL values missing\n",
                "df_dropped_all = df_dirty.dropna(how='all')\n",
                "print(f\"After dropna(how='all'): {df_dropped_all.shape}\")\n",
                "\n",
                "# Drop rows with missing in specific columns\n",
                "df_dropped_subset = df_dirty.dropna(subset=['number_of_purchases', 'total_transaction_amount'])\n",
                "print(f\"After dropna(subset=['number_of_purchases', 'total_transaction_amount']): {df_dropped_subset.shape}\")\n",
                "\n",
                "# Drop columns with missing values\n",
                "df_dropped_cols = df_dirty.dropna(axis=1)\n",
                "print(f\"After dropping columns with missing: {df_dropped_cols.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Filling Missing Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill numeric columns with a constant value\n",
                "df_filled_zero = df_dirty.fillna({'number_of_purchases': 0, 'total_transaction_amount': 0})\n",
                "print(\"Filled numerics with 0 - Missing count:\", df_filled_zero.isnull().sum().sum())\n",
                "\n",
                "# Fill with column mean\n",
                "numeric_cols = df_dirty.select_dtypes(include=[np.number]).columns\n",
                "df_filled_mean = df_dirty.copy()\n",
                "df_filled_mean[numeric_cols] = df_filled_mean[numeric_cols].fillna(df_filled_mean[numeric_cols].mean())\n",
                "print(\"Filled with mean - Missing count:\", df_filled_mean.isnull().sum().sum())\n",
                "\n",
                "# Fill with column median\n",
                "df_filled_median = df_dirty.copy()\n",
                "df_filled_median[numeric_cols] = df_filled_median[numeric_cols].fillna(df_filled_median[numeric_cols].median())\n",
                "print(\"Filled with median - Missing count:\", df_filled_median.isnull().sum().sum())\n",
                "\n",
                "# Fill different columns with different strategies\n",
                "df_custom_fill = df_dirty.copy()\n",
                "df_custom_fill['number_of_purchases'] = df_custom_fill['number_of_purchases'].fillna(df_custom_fill['number_of_purchases'].mean())\n",
                "df_custom_fill['total_transaction_amount'] = df_custom_fill['total_transaction_amount'].fillna(df_custom_fill['total_transaction_amount'].median())\n",
                "df_custom_fill['region'] = df_custom_fill['region'].fillna(df_custom_fill['region'].mode()[0])\n",
                "print(\"Custom fill - Missing count:\", df_custom_fill.isnull().sum().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Forward Fill and Backward Fill\n",
                "\n",
                "Useful for sequential data (e.g., customers appear in order — propagate last known name/region)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forward fill (propagate last valid observation forward)\n",
                "df_ffill = df_dirty.ffill()\n",
                "print(\"Forward fill - Missing count:\", df_ffill.isnull().sum().sum())\n",
                "\n",
                "# Backward fill (use next valid observation to fill gap)\n",
                "df_bfill = df_dirty.bfill()\n",
                "print(\"Backward fill - Missing count:\", df_bfill.isnull().sum().sum())\n",
                "\n",
                "# Combine forward and backward fill\n",
                "df_both_fill = df_dirty.ffill().bfill()\n",
                "print(\"Both fills - Missing count:\", df_both_fill.isnull().sum().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Interpolation\n",
                "\n",
                "For numeric columns, interpolation estimates missing values from neighbouring values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Linear interpolation on numeric columns\n",
                "df_interpolated = df_dirty.copy()\n",
                "df_interpolated[numeric_cols] = df_interpolated[numeric_cols].interpolate(method='linear')\n",
                "print(\"Linear interpolation - Missing count:\", df_interpolated.isnull().sum().sum())\n",
                "\n",
                "# Show example: total_transaction_amount before and after interpolation\n",
                "sample_col = 'total_transaction_amount'\n",
                "missing_idx = df_dirty[sample_col].isnull()\n",
                "if missing_idx.sum() > 0:\n",
                "    sample_idx = df_dirty[sample_col].isnull().idxmax()\n",
                "    window = slice(max(0, sample_idx - 2), min(len(df_dirty), sample_idx + 3))\n",
                "    comparison = pd.DataFrame({\n",
                "        'Original': df_dirty.loc[window, sample_col],\n",
                "        'Interpolated': df_interpolated.loc[window, sample_col]\n",
                "    })\n",
                "    print(f\"\\nInterpolation example for {sample_col}:\")\n",
                "    print(comparison)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Handling Duplicates\n",
                "\n",
                "### 4.1 Detecting Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add some duplicate rows for demonstration\n",
                "df_with_dupes = df.copy()\n",
                "df_with_dupes = pd.concat([df_with_dupes, df.head(10), df.iloc[50:55]], ignore_index=True)\n",
                "\n",
                "print(f\"Original dataset size: {len(df)}\")\n",
                "print(f\"With duplicates: {len(df_with_dupes)}\")\n",
                "\n",
                "# Check for duplicates\n",
                "duplicates = df_with_dupes.duplicated()\n",
                "print(f\"\\nNumber of duplicate rows: {duplicates.sum()}\")\n",
                "\n",
                "# Show duplicate rows\n",
                "print(\"\\nDuplicate rows:\")\n",
                "df_with_dupes[duplicates].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check duplicates based on specific columns (a customer-quarter pair should be unique)\n",
                "dupes_on_key = df_with_dupes.duplicated(subset=['customer_id', 'quarter'])\n",
                "print(f\"Duplicates based on customer_id + quarter: {dupes_on_key.sum()}\")\n",
                "\n",
                "# Keep first occurrence\n",
                "dupes_keep_first = df_with_dupes.duplicated(keep='first')\n",
                "print(f\"Duplicates (keep first): {dupes_keep_first.sum()}\")\n",
                "\n",
                "# Keep last occurrence\n",
                "dupes_keep_last = df_with_dupes.duplicated(keep='last')\n",
                "print(f\"Duplicates (keep last): {dupes_keep_last.sum()}\")\n",
                "\n",
                "# Mark all duplicates\n",
                "dupes_keep_false = df_with_dupes.duplicated(keep=False)\n",
                "print(f\"All duplicate rows (including first): {dupes_keep_false.sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Removing Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop duplicates (keep first)\n",
                "df_no_dupes = df_with_dupes.drop_duplicates()\n",
                "print(f\"After removing duplicates: {len(df_no_dupes)}\")\n",
                "\n",
                "# Drop duplicates based on customer-quarter key\n",
                "df_no_dupes_subset = df_with_dupes.drop_duplicates(subset=['customer_id', 'quarter'])\n",
                "print(f\"After removing duplicates on customer_id/quarter: {len(df_no_dupes_subset)}\")\n",
                "\n",
                "# Keep last occurrence\n",
                "df_keep_last = df_with_dupes.drop_duplicates(keep='last')\n",
                "print(f\"Keeping last duplicate: {len(df_keep_last)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Data Validation and Quality Checks\n",
                "\n",
                "### 5.1 Value Range Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for negative values (shouldn't exist in purchases/amounts)\n",
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "\n",
                "print(\"Checking for negative values:\")\n",
                "for col in numeric_cols:\n",
                "    negative_count = (df[col] < 0).sum()\n",
                "    if negative_count > 0:\n",
                "        print(f\"{col}: {negative_count} negative values\")\n",
                "    else:\n",
                "        print(f\"{col}: ✓ All non-negative\")\n",
                "\n",
                "# Check for outliers using IQR\n",
                "def find_outliers_iqr(data, column):\n",
                "    Q1 = data[column].quantile(0.25)\n",
                "    Q3 = data[column].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
                "    return outliers, lower_bound, upper_bound\n",
                "\n",
                "print(\"\\nOutlier detection for total_transaction_amount:\")\n",
                "outliers, lower, upper = find_outliers_iqr(df.dropna(subset=['total_transaction_amount']), 'total_transaction_amount')\n",
                "print(f\"IQR Range: [{lower:.2f}, {upper:.2f}]\")\n",
                "print(f\"Outliers found: {len(outliers)}\")\n",
                "if len(outliers) > 0:\n",
                "    print(outliers[['customer_id', 'region', 'quarter', 'total_transaction_amount']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Data Type Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check data types\n",
                "print(\"Current data types:\")\n",
                "print(df.dtypes)\n",
                "\n",
                "# Ensure numeric columns are actually numeric\n",
                "df_validated = df.copy()\n",
                "for col in numeric_cols:\n",
                "    df_validated[col] = pd.to_numeric(df_validated[col], errors='coerce')\n",
                "\n",
                "print(\"\\nAfter validation:\")\n",
                "print(df_validated.dtypes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Unique Value Counts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check unique values in each column\n",
                "print(\"Unique value counts:\")\n",
                "for col in df.columns:\n",
                "    unique_count = df[col].nunique()\n",
                "    print(f\"{col}: {unique_count} unique values\")\n",
                "\n",
                "# Categorical column distributions\n",
                "print(\"\\nRegion distribution:\")\n",
                "print(df['region'].value_counts())\n",
                "\n",
                "print(\"\\nQuarter distribution:\")\n",
                "print(df['quarter'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Data Transformation\n",
                "\n",
                "### 6.1 Creating New Columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_transformed = df.dropna(subset=['number_of_purchases', 'total_transaction_amount']).copy()\n",
                "\n",
                "# Average spend per purchase\n",
                "df_transformed['Avg_Per_Purchase'] = (\n",
                "    df_transformed['total_transaction_amount'] / df_transformed['number_of_purchases'].replace(0, np.nan)\n",
                ")\n",
                "\n",
                "# Boolean flag for high-value customer\n",
                "df_transformed['Is_High_Value'] = df_transformed['total_transaction_amount'] > 10000\n",
                "\n",
                "# Estimated annual spend\n",
                "df_transformed['Annual_Estimate'] = df_transformed['total_transaction_amount'] * 4\n",
                "\n",
                "# Revenue contribution percentage per quarter\n",
                "total_per_quarter = df_transformed.groupby('quarter')['total_transaction_amount'].transform('sum')\n",
                "df_transformed['Revenue_Share_Pct'] = (df_transformed['total_transaction_amount'] / total_per_quarter) * 100\n",
                "\n",
                "print(\"New columns created:\")\n",
                "print(df_transformed[['customer_id', 'number_of_purchases', 'total_transaction_amount',\n",
                "                       'Avg_Per_Purchase', 'Is_High_Value', 'Annual_Estimate']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Binning Continuous Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create spend categories\n",
                "df_transformed['Spend_Category'] = pd.cut(\n",
                "    df_transformed['total_transaction_amount'],\n",
                "    bins=[0, 3000, 8000, 15001],\n",
                "    labels=['Low', 'Medium', 'High']\n",
                ")\n",
                "\n",
                "# Create purchase frequency categories\n",
                "df_transformed['Freq_Group'] = pd.cut(\n",
                "    df_transformed['number_of_purchases'],\n",
                "    bins=[-1, 4, 14, 25],\n",
                "    labels=['Infrequent', 'Regular', 'Frequent']\n",
                ")\n",
                "\n",
                "# Equal-frequency binning (quantiles)\n",
                "df_transformed['Amount_Quartile'] = pd.qcut(\n",
                "    df_transformed['total_transaction_amount'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
                ")\n",
                "\n",
                "print(\"Binning results:\")\n",
                "print(df_transformed[['total_transaction_amount', 'Spend_Category',\n",
                "                       'number_of_purchases', 'Freq_Group', 'Amount_Quartile']].head(10))\n",
                "\n",
                "print(\"\\nSpend Category distribution:\")\n",
                "print(df_transformed['Spend_Category'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Apply Custom Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply function using entire row (axis=1)\n",
                "def classify_customer(row):\n",
                "    if row['total_transaction_amount'] > 12000:\n",
                "        return 'VIP'\n",
                "    elif row['number_of_purchases'] > 15:\n",
                "        return 'Frequent'\n",
                "    else:\n",
                "        return 'Standard'\n",
                "\n",
                "df_transformed['Customer_Class'] = df_transformed.apply(classify_customer, axis=1)\n",
                "\n",
                "# Lambda functions\n",
                "df_transformed['Big_Spender'] = df_transformed['total_transaction_amount'].apply(\n",
                "    lambda x: 'Yes' if x > 10000 else 'No'\n",
                ")\n",
                "\n",
                "# Map function: map quarter codes to readable labels\n",
                "quarter_map = {'Q1-2025': 'Jan-Mar', 'Q2-2025': 'Apr-Jun',\n",
                "               'Q3-2025': 'Jul-Sep', 'Q4-2025': 'Oct-Dec'}\n",
                "df_transformed['Quarter_Label'] = df_transformed['quarter'].map(quarter_map)\n",
                "\n",
                "print(\"Custom transformations:\")\n",
                "print(df_transformed[['total_transaction_amount', 'number_of_purchases',\n",
                "                       'Customer_Class', 'Big_Spender', 'Quarter_Label']].head(10))\n",
                "\n",
                "print(\"\\nCustomer Class distribution:\")\n",
                "print(df_transformed['Customer_Class'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Grouping and Aggregation\n",
                "\n",
                "### 7.1 Basic GroupBy Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Group by region and calculate mean transaction amount\n",
                "region_avg = df_transformed.groupby('region')['total_transaction_amount'].mean().sort_values(ascending=False)\n",
                "print(\"Mean transaction amount by region:\")\n",
                "print(region_avg.round(2))\n",
                "\n",
                "# Multiple aggregations\n",
                "quarter_stats = df_transformed.groupby('quarter')['total_transaction_amount'].agg(\n",
                "    ['count', 'mean', 'std', 'min', 'max']\n",
                ")\n",
                "print(\"\\nTransaction statistics by quarter:\")\n",
                "print(quarter_stats.round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Group by categorical variable\n",
                "category_stats = df_transformed.groupby('Spend_Category').agg({\n",
                "    'total_transaction_amount': ['mean', 'std', 'count'],\n",
                "    'number_of_purchases': 'mean'\n",
                "})\n",
                "print(\"Statistics by Spend Category:\")\n",
                "print(category_stats.round(2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Advanced Aggregations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multiple groupby columns\n",
                "region_quarter_stats = df_transformed.groupby(['region', 'quarter'])['total_transaction_amount'].agg(\n",
                "    ['count', 'mean', 'std']\n",
                ").round(2)\n",
                "print(\"Transaction Amount by Region and Quarter:\")\n",
                "print(region_quarter_stats.head(12))\n",
                "\n",
                "# Custom aggregation function\n",
                "def coefficient_of_variation(x):\n",
                "    return (x.std() / x.mean()) * 100 if x.mean() != 0 else 0\n",
                "\n",
                "cv_stats = df_transformed.groupby('region')['total_transaction_amount'].agg([\n",
                "    ('Mean', 'mean'),\n",
                "    ('StdDev', 'std'),\n",
                "    ('CV%', coefficient_of_variation)\n",
                "]).round(2)\n",
                "print(\"\\nCoefficient of Variation by Region:\")\n",
                "print(cv_stats.head(8))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 Transform and Filter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform: add group mean to each row (region average)\n",
                "df_transformed['Region_Mean_Amount'] = df_transformed.groupby('region')['total_transaction_amount'].transform('mean')\n",
                "df_transformed['Amount_vs_Region_Mean'] = df_transformed['total_transaction_amount'] - df_transformed['Region_Mean_Amount']\n",
                "\n",
                "print(\"Transform example (how each transaction compares to its region average):\")\n",
                "print(df_transformed[['customer_id', 'region', 'total_transaction_amount',\n",
                "                       'Region_Mean_Amount', 'Amount_vs_Region_Mean']].head(10).round(2))\n",
                "\n",
                "# Filter: keep only regions with >= 20 transactions in dataset\n",
                "regions_large = df_transformed.groupby('region').filter(lambda x: len(x) >= 20)\n",
                "print(f\"\\nOriginal rows: {len(df_transformed)}\")\n",
                "print(f\"After filtering (regions with >= 20 samples): {len(regions_large)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Reshaping Data\n",
                "\n",
                "### 8.1 Pivot Tables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create pivot table: average transaction amount per region x quarter\n",
                "pivot = df_transformed.pivot_table(\n",
                "    values='total_transaction_amount',\n",
                "    index='region',\n",
                "    columns='quarter',\n",
                "    aggfunc='mean'\n",
                ").round(2)\n",
                "\n",
                "print(\"Pivot Table - Mean Transaction Amount (Region × Quarter):\")\n",
                "print(pivot)\n",
                "\n",
                "# Multiple aggregation functions\n",
                "pivot_multi = df_transformed.pivot_table(\n",
                "    values='total_transaction_amount',\n",
                "    index='region',\n",
                "    columns='quarter',\n",
                "    aggfunc=['mean', 'count'],\n",
                "    fill_value=0\n",
                ").round(2)\n",
                "\n",
                "print(\"\\nPivot Table with Multiple Aggregations:\")\n",
                "print(pivot_multi.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2 Melting (Wide to Long)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate melting with summary data\n",
                "# Create a wide summary: one row per customer, one column per quarter\n",
                "wide_summary = df.dropna(subset=['total_transaction_amount']).pivot_table(\n",
                "    values='total_transaction_amount',\n",
                "    index='customer_id',\n",
                "    columns='quarter',\n",
                "    aggfunc='sum',\n",
                "    fill_value=0\n",
                ").head(5).reset_index()\n",
                "\n",
                "print(\"Wide format (one row per customer, one column per quarter):\")\n",
                "print(wide_summary)\n",
                "\n",
                "# Melt back to long format\n",
                "melted = wide_summary.melt(\n",
                "    id_vars=['customer_id'],\n",
                "    var_name='quarter',\n",
                "    value_name='total_transaction_amount'\n",
                ")\n",
                "\n",
                "print(\"\\nMelted (Long format):\")\n",
                "print(melted.sort_values(['customer_id', 'quarter']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Combining Datasets\n",
                "\n",
                "### 9.1 Concatenation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into quarterly chunks\n",
                "q1_data = df[df['quarter'] == 'Q1-2025'].copy()\n",
                "q2_data = df[df['quarter'] == 'Q2-2025'].copy()\n",
                "q3_data = df[df['quarter'] == 'Q3-2025'].copy()\n",
                "\n",
                "# Concatenate vertically (stack rows)\n",
                "stacked = pd.concat([q1_data, q2_data, q3_data], axis=0, ignore_index=True)\n",
                "print(f\"Q1: {len(q1_data)}, Q2: {len(q2_data)}, Q3: {len(q3_data)}\")\n",
                "print(f\"Stacked vertically: {stacked.shape}\")\n",
                "\n",
                "# Concatenate horizontally\n",
                "part1 = df[['customer_id', 'name']].head(10)\n",
                "part2 = df[['region', 'quarter', 'total_transaction_amount']].head(10)\n",
                "combined = pd.concat([part1, part2], axis=1)\n",
                "print(f\"\\nCombined horizontally: {combined.shape}\")\n",
                "print(combined.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9.2 Merging DataFrames"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample datasets to demonstrate merging\n",
                "# Customer profile table\n",
                "df_profiles = pd.DataFrame({\n",
                "    'customer_id': ['CUST00001', 'CUST00002', 'CUST00003', 'CUST00004'],\n",
                "    'membership_tier': ['Gold', 'Silver', 'Gold', 'Bronze'],\n",
                "    'signup_year': [2020, 2021, 2019, 2022]\n",
                "})\n",
                "\n",
                "# Transaction summary table\n",
                "df_summary = pd.DataFrame({\n",
                "    'customer_id': ['CUST00001', 'CUST00002', 'CUST00003', 'CUST00005'],\n",
                "    'total_annual_spend': [35000, 22000, 41000, 15000]\n",
                "})\n",
                "\n",
                "print(\"Customer Profiles:\")\n",
                "print(df_profiles)\n",
                "print(\"\\nTransaction Summary:\")\n",
                "print(df_summary)\n",
                "\n",
                "# Inner join (only matching customer_ids)\n",
                "inner = pd.merge(df_profiles, df_summary, on='customer_id', how='inner')\n",
                "print(\"\\nInner Join (matching customers only):\")\n",
                "print(inner)\n",
                "\n",
                "# Left join (all profiles, fill missing spend with NaN)\n",
                "left = pd.merge(df_profiles, df_summary, on='customer_id', how='left')\n",
                "print(\"\\nLeft Join (all profiles):\")\n",
                "print(left)\n",
                "\n",
                "# Outer join (all from both)\n",
                "outer = pd.merge(df_profiles, df_summary, on='customer_id', how='outer')\n",
                "print(\"\\nOuter Join (all customers):\")\n",
                "print(outer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. String Operations\n",
                "\n",
                "### 10.1 Working with Text Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Work on the customer_id and name columns\n",
                "df_text = df.dropna(subset=['name']).head(20).copy()\n",
                "\n",
                "# Create a descriptive ID string\n",
                "df_text['Transaction_Code'] = df_text.apply(\n",
                "    lambda x: f\"{x['customer_id']}_{x['quarter'].replace('-', '_')}\",\n",
                "    axis=1\n",
                ")\n",
                "\n",
                "print(\"Sample Transaction Codes:\")\n",
                "print(df_text['Transaction_Code'].head(10))\n",
                "\n",
                "# Extract customer number from customer_id\n",
                "df_text['Customer_Number'] = df_text['customer_id'].str.extract(r'(\\d+)')[0]\n",
                "\n",
                "# Extract first name\n",
                "df_text['First_Name'] = df_text['name'].str.split().str[0]\n",
                "\n",
                "print(\"\\nExtracted information:\")\n",
                "print(df_text[['customer_id', 'Customer_Number', 'name', 'First_Name']].head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# String operations\n",
                "df_text['ID_Lower'] = df_text['customer_id'].str.lower()\n",
                "df_text['ID_Upper'] = df_text['customer_id'].str.upper()\n",
                "df_text['Contains_001'] = df_text['customer_id'].str.contains('001')\n",
                "df_text['Starts_CUST'] = df_text['customer_id'].str.startswith('CUST')\n",
                "df_text['Split_Parts'] = df_text['Transaction_Code'].str.split('_')\n",
                "\n",
                "print(\"String operations:\")\n",
                "print(df_text[['customer_id', 'ID_Lower', 'Contains_001', 'Split_Parts']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 11. Sorting and Ranking\n",
                "\n",
                "### 11.1 Sorting Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sorted = df.dropna(subset=['total_transaction_amount']).copy()\n",
                "\n",
                "# Sort by single column\n",
                "top_spenders = df_sorted.sort_values('total_transaction_amount', ascending=False)\n",
                "print(\"Top 5 highest-spending transactions:\")\n",
                "print(top_spenders[['customer_id', 'name', 'region', 'quarter', 'total_transaction_amount']].head())\n",
                "\n",
                "# Sort by multiple columns\n",
                "sorted_multi = df_sorted.sort_values(['region', 'total_transaction_amount'], ascending=[True, False])\n",
                "print(\"\\nSorted by Region (asc) then Amount (desc):\")\n",
                "print(sorted_multi[['region', 'customer_id', 'quarter', 'total_transaction_amount']].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11.2 Ranking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create ranks\n",
                "df_ranked = df_sorted.copy()\n",
                "df_ranked['Amount_Rank'] = df_ranked['total_transaction_amount'].rank(ascending=False).astype(int)\n",
                "df_ranked['Amount_Percentile'] = df_ranked['total_transaction_amount'].rank(pct=True) * 100\n",
                "\n",
                "# Rank within groups (rank within each region)\n",
                "df_ranked['Region_Rank'] = df_ranked.groupby('region')['total_transaction_amount'].rank(ascending=False)\n",
                "\n",
                "print(\"Ranking examples:\")\n",
                "print(df_ranked[['customer_id', 'region', 'total_transaction_amount',\n",
                "                  'Amount_Rank', 'Amount_Percentile', 'Region_Rank']].head(15).round(1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 12. Comprehensive Data Cleaning Exercise\n",
                "\n",
                "**Task**: Clean and prepare a messy version of the customer transactions dataset:\n",
                "1. Handle missing values appropriately\n",
                "2. Remove duplicates\n",
                "3. Validate data ranges\n",
                "4. Create meaningful features\n",
                "5. Categorize data\n",
                "6. Produce summary statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create messy dataset from the real one\n",
                "np.random.seed(42)\n",
                "df_messy = df.copy()\n",
                "\n",
                "# Add extra missing values\n",
                "for col in ['number_of_purchases', 'total_transaction_amount']:\n",
                "    mask = np.random.random(len(df_messy)) < 0.06\n",
                "    df_messy.loc[mask, col] = np.nan\n",
                "\n",
                "# Add duplicates\n",
                "df_messy = pd.concat([df_messy, df_messy.sample(20)], ignore_index=True)\n",
                "\n",
                "# Add some errors (negative amounts — invalid)\n",
                "df_messy.loc[df_messy.sample(5).index, 'total_transaction_amount'] = -999\n",
                "\n",
                "print(\"Messy dataset created:\")\n",
                "print(f\"Shape: {df_messy.shape}\")\n",
                "print(f\"Missing values: {df_messy.isnull().sum().sum()}\")\n",
                "print(f\"Duplicates: {df_messy.duplicated().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SOLUTION\n",
                "\n",
                "# Step 1: Handle missing values\n",
                "df_clean = df_messy.copy()\n",
                "print(\"Step 1: Handling missing values\")\n",
                "print(f\"Before: {df_clean.isnull().sum().sum()} missing values\")\n",
                "\n",
                "df_clean['number_of_purchases'] = df_clean['number_of_purchases'].fillna(\n",
                "    df_clean['number_of_purchases'].median()\n",
                ")\n",
                "df_clean['total_transaction_amount'] = df_clean['total_transaction_amount'].fillna(\n",
                "    df_clean['total_transaction_amount'].median()\n",
                ")\n",
                "df_clean['name'] = df_clean['name'].ffill()\n",
                "df_clean['region'] = df_clean['region'].ffill()\n",
                "\n",
                "print(f\"After: {df_clean.isnull().sum().sum()} missing values\")\n",
                "\n",
                "# Step 2: Remove duplicates\n",
                "print(f\"\\nStep 2: Removing duplicates\")\n",
                "print(f\"Before: {len(df_clean)} rows\")\n",
                "df_clean = df_clean.drop_duplicates()\n",
                "print(f\"After: {len(df_clean)} rows\")\n",
                "\n",
                "# Step 3: Validate data ranges\n",
                "print(f\"\\nStep 3: Validating data ranges\")\n",
                "print(f\"Invalid amount values (negative): {(df_clean['total_transaction_amount'] < 0).sum()}\")\n",
                "df_clean = df_clean[df_clean['total_transaction_amount'] >= 0]\n",
                "df_clean = df_clean[df_clean['number_of_purchases'] >= 0]\n",
                "print(f\"After validation: {len(df_clean)} rows\")\n",
                "\n",
                "# Step 4: Create features\n",
                "print(f\"\\nStep 4: Creating features\")\n",
                "df_clean['Avg_Per_Purchase'] = (\n",
                "    df_clean['total_transaction_amount'] / df_clean['number_of_purchases'].replace(0, np.nan)\n",
                ")\n",
                "df_clean['Annual_Estimate'] = df_clean['total_transaction_amount'] * 4\n",
                "print(\"Features created: Avg_Per_Purchase, Annual_Estimate\")\n",
                "\n",
                "# Step 5: Categorize\n",
                "print(f\"\\nStep 5: Categorizing data\")\n",
                "df_clean['Spend_Category'] = pd.cut(\n",
                "    df_clean['total_transaction_amount'],\n",
                "    bins=[0, 3000, 8000, 15001],\n",
                "    labels=['Low', 'Medium', 'High']\n",
                ")\n",
                "df_clean['Customer_Tier'] = df_clean['total_transaction_amount'].apply(\n",
                "    lambda x: 'VIP' if x > 12000 else ('Regular' if x >= 5000 else 'Occasional')\n",
                ")\n",
                "print(\"Categories created: Spend_Category, Customer_Tier\")\n",
                "\n",
                "# Step 6: Summary statistics\n",
                "print(f\"\\nStep 6: Summary Statistics\")\n",
                "print(\"\\n=== Final Cleaned Dataset ===\")\n",
                "print(f\"Shape: {df_clean.shape}\")\n",
                "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
                "print(f\"Duplicates: {df_clean.duplicated().sum()}\")\n",
                "\n",
                "print(\"\\nMean transaction amount by region:\")\n",
                "print(df_clean.groupby('region')['total_transaction_amount'].agg(['count', 'mean', 'std']).round(2))\n",
                "\n",
                "print(\"\\nCustomer Tier distribution:\")\n",
                "print(df_clean['Customer_Tier'].value_counts())\n",
                "\n",
                "print(\"\\n✅ Data cleaning complete!\")\n",
                "df_clean.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Key Pandas Techniques Covered:\n",
                "\n",
                "**Data Quality**\n",
                "- ✅ Missing data detection and visualization\n",
                "- ✅ Duplicate identification and removal\n",
                "- ✅ Data validation and quality checks\n",
                "\n",
                "**Data Handling**\n",
                "- ✅ Multiple strategies for handling missing data (drop, fill, interpolate)\n",
                "- ✅ Forward fill, backward fill methods\n",
                "- ✅ String operations and text processing\n",
                "\n",
                "**Data Transformation**\n",
                "- ✅ Creating derived features (Avg_Per_Purchase, Annual_Estimate, Is_High_Value)\n",
                "- ✅ Binning and categorization (Spend_Category, Freq_Group)\n",
                "- ✅ Applying custom functions (classify_customer, Customer_Tier)\n",
                "- ✅ Sorting and ranking\n",
                "\n",
                "**Data Combination**\n",
                "- ✅ Concatenating quarterly datasets\n",
                "- ✅ Merging with customer profiles (inner, left, outer joins)\n",
                "- ✅ Reshaping (pivot region×quarter, melt wide→long)\n",
                "\n",
                "**Data Aggregation**\n",
                "- ✅ GroupBy by region and quarter\n",
                "- ✅ Multiple aggregation functions\n",
                "- ✅ Transform (region mean) and filter on groups\n",
                "- ✅ Pivot tables\n",
                "\n",
                "### Best Practices:\n",
                "1. Always inspect your data first (`head()`, `info()`, `describe()`)\n",
                "2. Check for missing values and duplicates\n",
                "3. Validate data ranges and types\n",
                "4. Work on a copy to preserve original data\n",
                "5. Document your cleaning decisions\n",
                "6. Create meaningful derived features\n",
                "7. Use appropriate aggregation methods"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}