{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Machine Learning with Scikit-Learn: Predicting Customer Transactions\n",
                "\n",
                "**Goal**: Predict `total_transaction_amount` for Malaysian customers.\n",
                "\n",
                "We will cover:\n",
                "1. **Data Preprocessing**: Encoding categoricals + `StandardScaler`.\n",
                "2. **Baseline Modeling**: Linear Regression as a benchmark.\n",
                "3. **Advanced Modeling**: Random Forest Regressor.\n",
                "4. **Robust Evaluation**: K-Fold Cross-Validation.\n",
                "5. **Hyperparameter Tuning**: `GridSearchCV`.\n",
                "\n",
                "Let's get started!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Load Data\n",
                "df = pd.read_csv(\"dummy_malaysia_customer_transactions_2025.csv\")\n",
                "\n",
                "# Encode categorical features\n",
                "le_region = LabelEncoder()\n",
                "le_quarter = LabelEncoder()\n",
                "df['region_encoded'] = le_region.fit_transform(df['region'].fillna('Unknown'))\n",
                "df['quarter_encoded'] = le_quarter.fit_transform(df['quarter'].fillna('Unknown'))\n",
                "\n",
                "features = ['region_encoded', 'quarter_encoded', 'number_of_purchases']\n",
                "target = 'total_transaction_amount'\n",
                "\n",
                "data = df[features + [target]].dropna()\n",
                "X = data[features]\n",
                "y = data[target]\n",
                "\n",
                "print(f\"Dataset loaded. Samples: {len(data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Feature Scaling\n",
                "\n",
                "We use `StandardScaler` (mean=0, std=1). Essential for distance-based algorithms (KNN, SVM, Neural Nets) and helps Linear Regression converge faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)\n",
                "print(\"Scaled Data Example:\")\n",
                "X_train_scaled_df.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline Model: Linear Regression\n",
                "\n",
                "We start with a simple model to set a performance benchmark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr = LinearRegression()\n",
                "lr.fit(X_train_scaled, y_train)\n",
                "y_pred_lr = lr.predict(X_test_scaled)\n",
                "\n",
                "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
                "r2_lr = r2_score(y_test, y_pred_lr)\n",
                "\n",
                "print(\"--- Baseline (Linear Regression) ---\")\n",
                "print(f\"MSE: {mse_lr:.2f}\")\n",
                "print(f\"R2 Score: {r2_lr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Advanced Model: Random Forest & Cross-Validation\n",
                "\n",
                "**Random Forest** can capture non-linear relationships. **K-Fold CV** (K=5) provides a more reliable accuracy estimate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='r2')\n",
                "\n",
                "print(\"--- Random Forest Cross-Validation ---\")\n",
                "print(f\"CV R2 Scores: {cv_scores}\")\n",
                "print(f\"Mean CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning with GridSearchCV\n",
                "\n",
                "`GridSearchCV` tries ALL combinations of parameters to find the best one:\n",
                "- `n_estimators`: Number of trees.\n",
                "- `max_depth`: Controls overfitting.\n",
                "- `min_samples_split`: Minimum samples to split a node."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "param_grid = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [None, 10, 20],\n",
                "    'min_samples_split': [2, 5]\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
                "print(\"Starting Grid Search... this may take a minute.\")\n",
                "grid_search.fit(X_train_scaled, y_train)\n",
                "\n",
                "best_rf = grid_search.best_estimator_\n",
                "print(\"\\n--- Optimization Complete ---\")\n",
                "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
                "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Evaluation\n",
                "\n",
                "Test the **Optimized Random Forest** on the unseen Test Set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_best = best_rf.predict(X_test_scaled)\n",
                "\n",
                "final_mse = mean_squared_error(y_test, y_pred_best)\n",
                "final_r2 = r2_score(y_test, y_pred_best)\n",
                "final_mae = mean_absolute_error(y_test, y_pred_best)\n",
                "\n",
                "print(\"--- Final Test Set Performance ---\")\n",
                "print(f\"Optimized Random Forest R2: {final_r2:.4f}\")\n",
                "print(f\"MAE: RM {final_mae:.2f}\")\n",
                "print(f\"Improvement over Baseline: {final_r2 - r2_lr:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization\n",
                "\n",
                "### 6.1 Actual vs Predicted Plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 8))\n",
                "plt.scatter(y_test, y_pred_best, alpha=0.6, color='purple', s=60)\n",
                "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                "plt.xlabel('Actual Transaction Amount (MYR)', fontsize=12)\n",
                "plt.ylabel('Predicted Transaction Amount (MYR)', fontsize=12)\n",
                "plt.title(f'Final Model Accuracy (R\\u00b2 = {final_r2:.2f})', fontsize=14)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Feature Importance\n",
                "What drives the transaction amount prediction in our optimized model?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "importances = best_rf.feature_importances_\n",
                "indices = np.argsort(importances)[::-1]\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "sns.barplot(x=importances[indices], y=np.array(features)[indices], palette='magma')\n",
                "plt.title('Feature Importance (Optimized Random Forest)', fontsize=14)\n",
                "plt.xlabel('Relative Importance')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Challenge Exercises"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task 1: Try Support Vector Regressor (SVR)\n",
                "# SVR is highly sensitive to scaling, so use X_train_scaled.\n",
                "# Import SVR from sklearn.svm\n",
                "# Experiment with kernel='rbf' or kernel='linear'\n",
                "\n",
                "# Your code here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task 2: Feature Engineering\n",
                "# Create 'Avg_Per_Purchase' = total_transaction_amount / number_of_purchases.\n",
                "# Add this to X, strictly re-split, re-scale, and train the model.\n",
                "# Does this domain-specific feature improve accuracy?\n",
                "\n",
                "# Your code here"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}