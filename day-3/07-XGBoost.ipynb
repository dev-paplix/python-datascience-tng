{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Boosting Predictions: XGBoost\n",
                "\n",
                "## Gradient Boosting Machines\n",
                "\n",
                "In the previous session, we used Random Forests, which build trees *independently*.\n",
                "Now we introduce **XGBoost (Extreme Gradient Boosting)**, which builds trees *sequentially* — each new tree corrects the errors of the previous ones.\n",
                "\n",
                "**Dataset**: Malaysia Customer Transactions 2025 — predicting `total_transaction_amount`.\n",
                "\n",
                "**Why XGBoost?**\n",
                "- **Speed**: Highly optimized for performance.\n",
                "- **Accuracy**: Often wins Kaggle competitions.\n",
                "- **Robustness**: Handles missing values internally and doesn't require explicit scaling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import xgboost as xgb\n",
                "\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "print(\"XGBoost version:\", xgb.__version__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "\n",
                "**Note on Scaling**: Unlike KNN or Neural Networks, tree-based models like XGBoost are *invariant* to feature scaling. We use the encoded data directly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "df = pd.read_csv(\"dummy_malaysia_customer_transactions_2025.csv\")\n",
                "\n",
                "# Encode categorical features\n",
                "le_region = LabelEncoder()\n",
                "le_quarter = LabelEncoder()\n",
                "df['region_encoded'] = le_region.fit_transform(df['region'].fillna('Unknown'))\n",
                "df['quarter_encoded'] = le_quarter.fit_transform(df['quarter'].fillna('Unknown'))\n",
                "\n",
                "# Select Features and Target\n",
                "features = ['region_encoded', 'quarter_encoded', 'number_of_purchases']\n",
                "target = 'total_transaction_amount'\n",
                "\n",
                "data = df[features + [target]].dropna()\n",
                "X = data[features]\n",
                "y = data[target]\n",
                "\n",
                "# Split Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Training Data Shape: {X_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline XGBoost Model\n",
                "\n",
                "We train a standard XGBoost Regressor.\n",
                "Key parameters:\n",
                "- `n_estimators`: Number of boosting rounds (trees).\n",
                "- `max_depth`: Depth of each tree.\n",
                "- `learning_rate` (or `eta`): Step size shrinkage to prevent overfitting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)\n",
                "\n",
                "# Train\n",
                "xgb_model.fit(X_train, y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred = xgb_model.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(\"--- Baseline XGBoost ---\")\n",
                "print(f\"MSE: {mse:.2f}\")\n",
                "print(f\"R2 Score: {r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dealing with Overfitting: Early Stopping\n",
                "\n",
                "**Early Stopping** lets us set `n_estimators` high and stop training if validation score doesn't improve for `N` rounds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
                "\n",
                "xgb_early = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, seed=42, early_stopping_rounds=10)\n",
                "xgb_early.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
                "\n",
                "print(f\"Best Iteration: {xgb_early.best_iteration}\")\n",
                "print(f\"Best Score (RMSE): {xgb_early.best_score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning\n",
                "\n",
                "XGBoost is powerful but sensitive to hyperparameters. We'll tune:\n",
                "- `learning_rate`: Lower is usually better (requires more trees).\n",
                "- `max_depth`: Shallow trees prevent overfitting.\n",
                "- `subsample`: Fraction of data samples used per tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "param_grid = {\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'learning_rate': [0.01, 0.1, 0.2],\n",
                "    'n_estimators': [100, 200]\n",
                "}\n",
                "\n",
                "xgb_tuned = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
                "grid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=3, scoring='r2', verbose=1)\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best Params: {grid_search.best_params_}\")\n",
                "print(f\"Best CV R2: {grid_search.best_score_:.4f}\")\n",
                "\n",
                "best_xgb = grid_search.best_estimator_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualization: Feature Importance\n",
                "\n",
                "XGBoost shows how often each feature is used to split data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 5))\n",
                "xgb.plot_importance(best_xgb, max_num_features=10, height=0.5)\n",
                "plt.title(\"XGBoost Feature Importance (Customer Transactions)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Exercises\n",
                "\n",
                "Challenge yourself with these tasks!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task 1: Tune 'min_child_weight' and 'gamma'\n",
                "# These parameters control regularization.\n",
                "# Add them to a new GridSearchCV and see if you can beat the previous best score.\n",
                "\n",
                "# Your code here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task 2: Compare Speed\n",
                "# Use Python's 'time' module to measure training time of Random Forest vs XGBoost.\n",
                "# Which one is faster for this dataset?\n",
                "\n",
                "import time\n",
                "# Your timing code here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "You've now added **XGBoost** to your ML toolkit!\n",
                "1. **Gradient Boosting** reduces bias by correcting previous errors.\n",
                "2. **Early Stopping** prevents overfitting automatically.\n",
                "3. **Tuning** unlocks the full potential of the model.\n",
                "\n",
                "Use XGBoost when you need top-tier accuracy on tabular data."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}