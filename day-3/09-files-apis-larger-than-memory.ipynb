{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 9: Files, APIs & Larger-than-Memory Tactics\n",
                "\n",
                "**Learning Objectives:**\n",
                "- Ingest large CSV files in chunks without running out of RAM\n",
                "- Use the Parquet format for faster, compressed data storage\n",
                "- Pull data from REST APIs into a Pandas workflow\n",
                "- Apply caching and profiling techniques to speed up ML pipelines\n",
                "\n",
                "> **Why does this matter?** Real-world datasets are large and live in many places — spreadsheets, APIs, databases. Knowing how to efficiently read, transform, and store them is what separates a production data scientist from a notebook prototype."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import io\n",
                "import time\n",
                "import json\n",
                "import gzip\n",
                "import pickle\n",
                "import hashlib\n",
                "import requests\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Create a working directory for this module\n",
                "os.makedirs('data', exist_ok=True)\n",
                "os.makedirs('cache', exist_ok=True)\n",
                "print(\"Working directories created: data/ and cache/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Generating a Large Synthetic Dataset\n",
                "\n",
                "Before practicing chunked ingestion, we need a big file to work with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write a 500k-row CSV to simulate a 'large' file\n",
                "np.random.seed(42)\n",
                "N = 500_000\n",
                "\n",
                "large_df = pd.DataFrame({\n",
                "    'transaction_id': range(1, N + 1),\n",
                "    'user_id': np.random.randint(1000, 9999, N),\n",
                "    'amount': np.random.exponential(scale=200, size=N).round(2),\n",
                "    'category': np.random.choice(['Food', 'Electronics', 'Clothing', 'Travel', 'Health'], N),\n",
                "    'status': np.random.choice(['completed', 'refunded', 'failed'], N, p=[0.85, 0.1, 0.05]),\n",
                "    'timestamp': pd.date_range('2022-01-01', periods=N, freq='1min').strftime('%Y-%m-%d %H:%M:%S')\n",
                "})\n",
                "\n",
                "csv_path = 'data/transactions_large.csv'\n",
                "large_df.to_csv(csv_path, index=False)\n",
                "file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
                "print(f\"Wrote {N:,} rows to '{csv_path}' ({file_size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Chunked CSV Ingestion\n",
                "\n",
                "When a file is too large to fit in RAM, we read and process it in **chunks**. Each chunk is a regular DataFrame — we process it and throw it away before reading the next one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1: Sum total revenue by category across all chunks\n",
                "CHUNK_SIZE = 50_000\n",
                "category_totals = {}\n",
                "\n",
                "start = time.time()\n",
                "with pd.read_csv(csv_path, chunksize=CHUNK_SIZE) as reader:\n",
                "    for i, chunk in enumerate(reader):\n",
                "        # Keep only completed transactions\n",
                "        chunk_filtered = chunk[chunk['status'] == 'completed']\n",
                "        # Accumulate totals\n",
                "        chunk_agg = chunk_filtered.groupby('category')['amount'].sum()\n",
                "        for cat, total in chunk_agg.items():\n",
                "            category_totals[cat] = category_totals.get(cat, 0) + total\n",
                "\n",
                "elapsed = time.time() - start\n",
                "result_df = pd.Series(category_totals).reset_index()\n",
                "result_df.columns = ['Category', 'Total Revenue ($)']\n",
                "result_df = result_df.sort_values('Total Revenue ($)', ascending=False)\n",
                "\n",
                "print(f\"Processed {N:,} rows in {elapsed:.2f}s using chunks of {CHUNK_SIZE:,}\")\n",
                "print(\"\\nRevenue by Category (completed transactions):\")\n",
                "result_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 2: Filter and save only 'refunded' rows\n",
                "refunded_chunks = []\n",
                "\n",
                "with pd.read_csv(csv_path, chunksize=CHUNK_SIZE) as reader:\n",
                "    for chunk in reader:\n",
                "        refunded_chunks.append(chunk[chunk['status'] == 'refunded'])\n",
                "\n",
                "refunded_df = pd.concat(refunded_chunks, ignore_index=True)\n",
                "print(f\"Total refunded transactions: {len(refunded_df):,}\")\n",
                "refunded_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reusable chunk processing function\n",
                "def process_csv_in_chunks(filepath, chunk_size, filter_fn=None, agg_fn=None):\n",
                "    \"\"\"\n",
                "    Generic chunked CSV processor.\n",
                "    - filter_fn: function(chunk) -> filtered_chunk\n",
                "    - agg_fn: function(chunk) -> value to accumulate\n",
                "    Returns a list of per-chunk results.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    with pd.read_csv(filepath, chunksize=chunk_size) as reader:\n",
                "        for chunk in reader:\n",
                "            if filter_fn:\n",
                "                chunk = filter_fn(chunk)\n",
                "            if agg_fn:\n",
                "                results.append(agg_fn(chunk))\n",
                "            else:\n",
                "                results.append(chunk)\n",
                "    return results\n",
                "\n",
                "# Example usage\n",
                "count_results = process_csv_in_chunks(\n",
                "    csv_path,\n",
                "    chunk_size=100_000,\n",
                "    filter_fn=lambda c: c[c['status'] == 'completed'],\n",
                "    agg_fn=lambda c: len(c)\n",
                ")\n",
                "print(f\"Total completed transactions: {sum(count_results):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Parquet & Compression\n",
                "\n",
                "Parquet is a **columnar storage format** that is:\n",
                "- Much faster to read (only loads requested columns)\n",
                "- Significantly smaller on disk (compressed by default)\n",
                "- Schema-aware (preserves dtypes perfectly)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the refunded dataset as our example\n",
                "csv_out_path = 'data/refunded.csv'\n",
                "parquet_path = 'data/refunded.parquet'\n",
                "parquet_gz_path = 'data/refunded_gzip.parquet'\n",
                "\n",
                "refunded_df.to_csv(csv_out_path, index=False)\n",
                "refunded_df.to_parquet(parquet_path, index=False)\n",
                "refunded_df.to_parquet(parquet_gz_path, index=False, compression='gzip')\n",
                "\n",
                "sizes = {\n",
                "    'CSV': os.path.getsize(csv_out_path) / 1024,\n",
                "    'Parquet (snappy)': os.path.getsize(parquet_path) / 1024,\n",
                "    'Parquet (gzip)': os.path.getsize(parquet_gz_path) / 1024\n",
                "}\n",
                "\n",
                "print(\"File size comparison:\")\n",
                "for fmt, kb in sizes.items():\n",
                "    print(f\"  {fmt:<22}: {kb:>8.1f} KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read speed comparison\n",
                "def time_read(path, read_fn, label):\n",
                "    start = time.time()\n",
                "    df = read_fn(path)\n",
                "    elapsed = time.time() - start\n",
                "    print(f\"{label:<28}: {elapsed:.4f}s  ({len(df):,} rows)\")\n",
                "    return df\n",
                "\n",
                "print(\"Read performance comparison:\")\n",
                "time_read(csv_out_path, pd.read_csv, 'CSV')\n",
                "time_read(parquet_path, pd.read_parquet, 'Parquet (snappy)')\n",
                "time_read(parquet_gz_path, pd.read_parquet, 'Parquet (gzip)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parquet column pruning — only load the columns you need (huge speed gain!)\n",
                "start = time.time()\n",
                "partial_df = pd.read_parquet(parquet_path, columns=['user_id', 'amount'])\n",
                "elapsed = time.time() - start\n",
                "print(f\"Parquet with column pruning (2 cols): {elapsed:.4f}s\")\n",
                "partial_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. API Ingestion Basics\n",
                "\n",
                "APIs (Application Programming Interfaces) are the most common way to pull fresh data from external services — financial data, weather, CRM systems, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1: Public JSON API (Open-Meteo weather — no auth required)\n",
                "def fetch_weather(latitude=1.3521, longitude=103.8198, days=7):\n",
                "    \"\"\"Fetch daily temperature forecast from Open-Meteo API.\"\"\"\n",
                "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
                "    params = {\n",
                "        'latitude': latitude,\n",
                "        'longitude': longitude,\n",
                "        'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum',\n",
                "        'timezone': 'Asia/Singapore',\n",
                "        'forecast_days': days\n",
                "    }\n",
                "    response = requests.get(url, params=params, timeout=10)\n",
                "    response.raise_for_status()  # Raises exception on HTTP error\n",
                "    return response.json()\n",
                "\n",
                "try:\n",
                "    weather_json = fetch_weather()\n",
                "    weather_df = pd.DataFrame(weather_json['daily'])\n",
                "    print(\"✅ Weather API call successful!\")\n",
                "    weather_df\n",
                "except Exception as e:\n",
                "    print(f\"⚠️  API call failed (expected offline): {e}\")\n",
                "    # Fallback mock data\n",
                "    weather_df = pd.DataFrame({\n",
                "        'time': pd.date_range('2024-01-01', periods=7, freq='D').strftime('%Y-%m-%d'),\n",
                "        'temperature_2m_max': [32.1, 33.5, 31.8, 34.0, 32.7, 33.2, 31.5],\n",
                "        'temperature_2m_min': [25.4, 26.1, 24.9, 26.5, 25.8, 26.0, 24.7],\n",
                "        'precipitation_sum': [0.2, 0.0, 5.4, 0.0, 2.1, 0.0, 0.0]\n",
                "    })\n",
                "    print(\"Using mock weather data instead.\")\n",
                "weather_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Robust API pattern: retry with exponential backoff\n",
                "def api_get_with_retry(url, params=None, max_retries=3, backoff=2.0):\n",
                "    \"\"\"GET request with retry logic and exponential backoff.\"\"\"\n",
                "    for attempt in range(1, max_retries + 1):\n",
                "        try:\n",
                "            response = requests.get(url, params=params, timeout=10)\n",
                "            response.raise_for_status()\n",
                "            return response.json()\n",
                "        except requests.exceptions.RequestException as e:\n",
                "            wait = backoff ** attempt\n",
                "            print(f\"Attempt {attempt}/{max_retries} failed: {e}. Retrying in {wait}s...\")\n",
                "            time.sleep(wait)\n",
                "    raise RuntimeError(f\"API call failed after {max_retries} attempts.\")\n",
                "\n",
                "print(\"api_get_with_retry() defined and ready to use.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pattern: ingest paginated API\n",
                "def fetch_paginated(base_url, page_param='page', per_page=100, max_pages=5):\n",
                "    \"\"\"\n",
                "    Yields DataFrames page by page from a paginated JSON API.\n",
                "    Stops when an empty page is returned or max_pages reached.\n",
                "    \"\"\"\n",
                "    for page in range(1, max_pages + 1):\n",
                "        params = {page_param: page, 'per_page': per_page}\n",
                "        try:\n",
                "            data = api_get_with_retry(base_url, params=params)\n",
                "            if not data:\n",
                "                print(f\"Empty page {page}, stopping.\")\n",
                "                break\n",
                "            yield pd.DataFrame(data)\n",
                "        except Exception as e:\n",
                "            print(f\"Stopped at page {page}: {e}\")\n",
                "            break\n",
                "\n",
                "# Usage example (JSONPlaceholder - free test API)\n",
                "frames = list(fetch_paginated('https://jsonplaceholder.typicode.com/posts', max_pages=2))\n",
                "if frames:\n",
                "    posts_df = pd.concat(frames, ignore_index=True)\n",
                "    print(f\"Fetched {len(posts_df)} posts from API.\")\n",
                "    posts_df[['userId', 'id', 'title']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Caching & Performance Tips for ML Pipelines\n",
                "\n",
                "Caching avoids re-running expensive operations (API calls, heavy data processing) every time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple disk-based cache using pickle\n",
                "def cache_key(fn_name, *args, **kwargs):\n",
                "    \"\"\"Generate a unique cache key from function name and arguments.\"\"\"\n",
                "    raw = f\"{fn_name}_{args}_{sorted(kwargs.items())}\"\n",
                "    return hashlib.md5(raw.encode()).hexdigest()\n",
                "\n",
                "def cached_call(fn, cache_dir='cache', ttl_seconds=3600, *args, **kwargs):\n",
                "    \"\"\"\n",
                "    Calls fn(*args, **kwargs), caching the result to disk.\n",
                "    Returns cached result if it exists and is within TTL.\n",
                "    \"\"\"\n",
                "    key = cache_key(fn.__name__, *args, **kwargs)\n",
                "    cache_file = os.path.join(cache_dir, f\"{key}.pkl\")\n",
                "    \n",
                "    if os.path.exists(cache_file):\n",
                "        age = time.time() - os.path.getmtime(cache_file)\n",
                "        if age < ttl_seconds:\n",
                "            print(f\"[CACHE HIT]  {fn.__name__} (age: {age:.0f}s)\")\n",
                "            with open(cache_file, 'rb') as f:\n",
                "                return pickle.load(f)\n",
                "    \n",
                "    print(f\"[CACHE MISS] Running {fn.__name__}...\")\n",
                "    result = fn(*args, **kwargs)\n",
                "    with open(cache_file, 'wb') as f:\n",
                "        pickle.dump(result, f)\n",
                "    return result\n",
                "\n",
                "\n",
                "# Demo: slow data loading function\n",
                "def load_and_aggregate_data(filepath):\n",
                "    time.sleep(1)  # Simulating expensive operation\n",
                "    df = pd.read_csv(filepath)\n",
                "    return df.groupby('category')['amount'].sum()\n",
                "\n",
                "# First call: cache miss\n",
                "result1 = cached_call(load_and_aggregate_data, 'cache', 3600, csv_path)\n",
                "\n",
                "# Second call: cache hit\n",
                "result2 = cached_call(load_and_aggregate_data, 'cache', 3600, csv_path)\n",
                "result2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory optimization: downcast numeric types\n",
                "def optimize_dtypes(df):\n",
                "    \"\"\"Reduce memory usage by downcasting numeric columns.\"\"\"\n",
                "    before = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
                "    \n",
                "    for col in df.select_dtypes(include=['float64']).columns:\n",
                "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
                "    for col in df.select_dtypes(include=['int64']).columns:\n",
                "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
                "    for col in df.select_dtypes(include=['object']).columns:\n",
                "        if df[col].nunique() / len(df) < 0.5:  # Low cardinality\n",
                "            df[col] = df[col].astype('category')\n",
                "    \n",
                "    after = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
                "    print(f\"Memory: {before:.2f} MB → {after:.2f} MB  ({(1-after/before)*100:.1f}% reduction)\")\n",
                "    return df\n",
                "\n",
                "sample = pd.read_csv(csv_path, nrows=100_000)\n",
                "sample_optimized = optimize_dtypes(sample.copy())\n",
                "sample_optimized.dtypes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Exercises\n",
                "\n",
                "1. Modify `process_csv_in_chunks` to calculate the **median** amount per category across 100k-row chunks. (*Hint: true median across chunks requires storing all values — why?*)\n",
                "2. Compare Parquet with the `brotli` compression codec. How does it compare to gzip in size and speed?\n",
                "3. Call the Open Library API at `https://openlibrary.org/search.json?q=python` and load the first 10 results into a DataFrame with columns: title, author, first_publish_year.\n",
                "4. Add a `from_date` parameter to the cache key in `cached_call` so the same function with different date args gets different cache files.\n",
                "5. Apply `optimize_dtypes` to the full 500k-row CSV and measure the total reduction."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}