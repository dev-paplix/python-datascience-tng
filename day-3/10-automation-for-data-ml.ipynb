{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 10: Automation for Data & ML (Expanded)\n",
                "\n",
                "**Learning Objectives:**\n",
                "- Schedule Python scripts using cron (Linux/Mac) and Task Scheduler (Windows)\n",
                "- Implement structured logging and error alerting\n",
                "- Organize data/ML projects with best-practice folder structures\n",
                "- Use n8n to orchestrate automated ML pipelines, dataset refreshes, and report delivery\n",
                "\n",
                "> **Why does this matter?** Running a notebook manually is fine for exploration. A production data system runs *automatically*, recovers from errors, and sends you an alert if something goes wrong."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "import logging\n",
                "import smtplib\n",
                "import textwrap\n",
                "import subprocess\n",
                "import traceback\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from email.message import EmailMessage\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Libraries loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Folder Structure Best Practices\n",
                "\n",
                "A well-organized project is the foundation of automation. Using a standard structure helps collaborators understand the project and allows automated scripts to find files reliably."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Canonical folder structure for an analytics/ML project\n",
                "PROJECT_STRUCTURE = \"\"\"\n",
                "my_project/\n",
                "â”œâ”€â”€ data/\n",
                "â”‚   â”œâ”€â”€ raw/             # Original, unmodified source files\n",
                "â”‚   â”œâ”€â”€ processed/       # Cleaned, feature-engineered data\n",
                "â”‚   â””â”€â”€ exports/         # Final outputs (reports, predictions)\n",
                "â”œâ”€â”€ notebooks/           # Exploratory Jupyter notebooks\n",
                "â”œâ”€â”€ src/\n",
                "â”‚   â”œâ”€â”€ __init__.py\n",
                "â”‚   â”œâ”€â”€ ingest.py        # Data ingestion functions\n",
                "â”‚   â”œâ”€â”€ clean.py         # Cleaning & validation functions\n",
                "â”‚   â”œâ”€â”€ features.py      # Feature engineering\n",
                "â”‚   â”œâ”€â”€ model.py         # Training, predict, evaluate\n",
                "â”‚   â””â”€â”€ report.py        # Report generation\n",
                "â”œâ”€â”€ scripts/\n",
                "â”‚   â”œâ”€â”€ run_pipeline.py  # Full pipeline entry point\n",
                "â”‚   â””â”€â”€ refresh_data.py  # Scheduled data refresh\n",
                "â”œâ”€â”€ logs/                # Rotating log files\n",
                "â”œâ”€â”€ config/\n",
                "â”‚   â””â”€â”€ config.json      # Environment-agnostic settings\n",
                "â”œâ”€â”€ models/              # Saved ML model artifacts\n",
                "â”œâ”€â”€ tests/               # Unit tests\n",
                "â”œâ”€â”€ requirements.txt\n",
                "â””â”€â”€ README.md\n",
                "\"\"\"\n",
                "print(PROJECT_STRUCTURE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaffold the structure programmatically\n",
                "def scaffold_project(base_path, name='my_project'):\n",
                "    root = Path(base_path) / name\n",
                "    dirs = [\n",
                "        'data/raw', 'data/processed', 'data/exports',\n",
                "        'notebooks', 'src', 'scripts', 'logs', 'config', 'models', 'tests'\n",
                "    ]\n",
                "    for d in dirs:\n",
                "        (root / d).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    # Init files\n",
                "    (root / 'src' / '__init__.py').touch()\n",
                "    (root / 'config' / 'config.json').write_text(json.dumps(\n",
                "        {'env': 'development', 'data_path': 'data/raw', 'log_level': 'INFO'}, indent=2\n",
                "    ))\n",
                "    (root / 'README.md').write_text(f\"# {name}\\n\\nProject description here.\\n\")\n",
                "    (root / 'requirements.txt').write_text(\"numpy\\npandas\\nscikit-learn\\nmatplotlib\\nseaborn\\nrequests\\n\")\n",
                "\n",
                "    print(f\"âœ… Project scaffolded at: {root.resolve()}\")\n",
                "    for p in sorted(root.rglob('*')):\n",
                "        indent = '  ' * (len(p.relative_to(root).parts) - 1)\n",
                "        print(f\"{indent}{'ðŸ“' if p.is_dir() else 'ðŸ“„'} {p.name}\")\n",
                "    return root\n",
                "\n",
                "project_root = scaffold_project('.', 'demo_project')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Structured Logging\n",
                "\n",
                "Using Python's `logging` module instead of `print()` gives you:\n",
                "- Severity levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
                "- Timestamps and source location\n",
                "- File rotation and multiple handlers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logging.handlers import RotatingFileHandler\n",
                "\n",
                "def setup_logger(name, log_file='logs/pipeline.log', level=logging.INFO):\n",
                "    \"\"\"Create a logger with console + rotating file handler.\"\"\"\n",
                "    os.makedirs('logs', exist_ok=True)\n",
                "    \n",
                "    logger = logging.getLogger(name)\n",
                "    logger.setLevel(level)\n",
                "    logger.handlers = []  # Clear existing handlers\n",
                "\n",
                "    fmt = logging.Formatter(\n",
                "        '%(asctime)s | %(name)s | %(levelname)-8s | %(message)s',\n",
                "        datefmt='%Y-%m-%d %H:%M:%S'\n",
                "    )\n",
                "\n",
                "    # Console handler\n",
                "    ch = logging.StreamHandler()\n",
                "    ch.setFormatter(fmt)\n",
                "    logger.addHandler(ch)\n",
                "\n",
                "    # Rotating file handler (10 MB max, 5 backups)\n",
                "    fh = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)\n",
                "    fh.setFormatter(fmt)\n",
                "    logger.addHandler(fh)\n",
                "\n",
                "    return logger\n",
                "\n",
                "logger = setup_logger('data_pipeline')\n",
                "logger.info('Pipeline logger initialized.')\n",
                "logger.warning('This is a warning example.')\n",
                "logger.error('This is a simulated error entry.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decorator: auto-log function calls with timing\n",
                "import functools\n",
                "\n",
                "def log_step(logger):\n",
                "    \"\"\"Decorator that logs a pipeline step's start, duration, and errors.\"\"\"\n",
                "    def decorator(fn):\n",
                "        @functools.wraps(fn)\n",
                "        def wrapper(*args, **kwargs):\n",
                "            logger.info(f\"â–¶ Starting: {fn.__name__}\")\n",
                "            start = time.time()\n",
                "            try:\n",
                "                result = fn(*args, **kwargs)\n",
                "                elapsed = time.time() - start\n",
                "                logger.info(f\"âœ… Completed: {fn.__name__} in {elapsed:.3f}s\")\n",
                "                return result\n",
                "            except Exception as e:\n",
                "                logger.error(f\"âŒ Failed: {fn.__name__} â€” {e}\")\n",
                "                logger.debug(traceback.format_exc())\n",
                "                raise\n",
                "        return wrapper\n",
                "    return decorator\n",
                "\n",
                "@log_step(logger)\n",
                "def load_raw_data(n=1000):\n",
                "    time.sleep(0.1)  # Simulate I/O\n",
                "    return pd.DataFrame({'x': np.random.randn(n), 'y': np.random.randn(n)})\n",
                "\n",
                "@log_step(logger)\n",
                "def clean_data(df):\n",
                "    return df.dropna()\n",
                "\n",
                "df = load_raw_data()\n",
                "df_clean = clean_data(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Error Handling & Alerts\n",
                "\n",
                "Production pipelines must gracefully handle failures and optionally notify humans."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generic pipeline error handler\n",
                "def safe_run(fn, *args, on_error=None, **kwargs):\n",
                "    \"\"\"\n",
                "    Wraps a function call. On exception, calls on_error(e) if provided.\n",
                "    Returns (result, error) tuple.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        return fn(*args, **kwargs), None\n",
                "    except Exception as e:\n",
                "        if on_error:\n",
                "            on_error(e)\n",
                "        return None, e\n",
                "\n",
                "def send_slack_alert(webhook_url, message):\n",
                "    \"\"\"Send error alert to a Slack webhook.\"\"\"\n",
                "    payload = {'text': f\":rotating_light: *Pipeline Alert*\\n{message}\"}\n",
                "    try:\n",
                "        r = requests.post(webhook_url, json=payload, timeout=5)\n",
                "        r.raise_for_status()\n",
                "        print(\"Slack alert sent.\")\n",
                "    except Exception as e:\n",
                "        logger.warning(f\"Could not send Slack alert: {e}\")\n",
                "\n",
                "# Example alert function (replace with your Slack webhook)\n",
                "SLACK_WEBHOOK = 'https://hooks.slack.com/YOUR_WEBHOOK_URL'\n",
                "\n",
                "def on_pipeline_error(e):\n",
                "    msg = f\"Error in data pipeline: `{type(e).__name__}: {e}`\"\n",
                "    logger.critical(msg)\n",
                "    # send_slack_alert(SLACK_WEBHOOK, msg)  # Uncomment when webhook is configured\n",
                "    print(f\"[ALERT TRIGGERED] {msg}\")\n",
                "\n",
                "# Test safe_run\n",
                "def risky_step():\n",
                "    raise ValueError(\"Source file missing from data/raw/!\")\n",
                "\n",
                "result, err = safe_run(risky_step, on_error=on_pipeline_error)\n",
                "print(f\"Result: {result} | Error caught: {err}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Scheduling: Cron (Linux/Mac) & Task Scheduler (Windows)\n",
                "\n",
                "### Linux/Mac â€” cron job\n",
                "```bash\n",
                "# Open crontab editor\n",
                "crontab -e\n",
                "\n",
                "# Run python script every day at 6:00 AM\n",
                "0 6 * * * /path/to/venv/bin/python /path/to/scripts/run_pipeline.py >> /path/to/logs/cron.log 2>&1\n",
                "\n",
                "# Run every Monday at 8:00 AM\n",
                "0 8 * * 1 /path/to/venv/bin/python /path/to/scripts/refresh_data.py\n",
                "```\n",
                "\n",
                "### Windows â€” Task Scheduler (via PowerShell)\n",
                "```powershell\n",
                "$action = New-ScheduledTaskAction -Execute 'python.exe' `\n",
                "    -Argument 'C:\\projects\\scripts\\run_pipeline.py' `\n",
                "    -WorkingDirectory 'C:\\projects'\n",
                "\n",
                "$trigger = New-ScheduledTaskTrigger -Daily -At '6:00AM'\n",
                "\n",
                "Register-ScheduledTask -Action $action -Trigger $trigger `\n",
                "    -TaskName 'DataPipeline' -Description 'Daily ML Pipeline'\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Portable scheduler using Python (no OS setup needed)\n",
                "# Install: pip install schedule\n",
                "\"\"\"\n",
                "import schedule\n",
                "import time\n",
                "\n",
                "def daily_pipeline():\n",
                "    print(f'[{datetime.now()}] Running daily pipeline...')\n",
                "    # ... your pipeline code here\n",
                "\n",
                "schedule.every().day.at('06:00').do(daily_pipeline)\n",
                "schedule.every().monday.at('08:00').do(daily_pipeline)\n",
                "\n",
                "while True:\n",
                "    schedule.run_pending()\n",
                "    time.sleep(60)  # Check every minute\n",
                "\"\"\"\n",
                "print(\"(Code shown for reference â€” run as a standalone script, not in a notebook)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. n8n for Automation\n",
                "\n",
                "**n8n** is an open-source workflow automation tool (like Zapier, but self-hosted). It lets you visually wire up triggers and actions â€” no code required for the glue.\n",
                "\n",
                "### Key n8n Concepts\n",
                "| Term | Meaning |\n",
                "|---|---|\n",
                "| **Workflow** | A visual sequence of nodes |\n",
                "| **Trigger Node** | What starts the workflow (schedule, webhook, email) |\n",
                "| **Action Node** | What happens (run Python, send email, call API) |\n",
                "| **Execute Command** | Node that runs a shell/Python command |\n",
                "| **Webhook Node** | HTTP endpoint that triggers a workflow |\n",
                "| **HTTP Request Node** | Calls any external REST API |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1  Auto-Run Python Scripts via n8n\n",
                "\n",
                "In n8n, create a workflow:\n",
                "```\n",
                "[Schedule Trigger: Daily 06:00] â†’ [Execute Command]\n",
                "```\n",
                "In the **Execute Command** node:\n",
                "```bash\n",
                "cd /home/user/projects && source venv/bin/activate && python scripts/run_pipeline.py\n",
                "```\n",
                "The output of the script (stdout/stderr) is available in subsequent nodes."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2  Auto-Refresh Datasets\n",
                "\n",
                "Workflow for refreshing a dataset from an API and saving to Parquet:\n",
                "```\n",
                "[Schedule Trigger: Every 1 Hour]\n",
                "  â†’ [HTTP Request: GET https://api.example.com/data]\n",
                "  â†’ [Execute Command: python -c \"import pandas as pd; ...\"]\n",
                "  â†’ [Write Binary File: data/processed/latest.parquet]\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3  Trigger ML Prediction Workflow\n",
                "\n",
                "Expose your ML model as a webhook endpoint (n8n receives data, runs the model, returns a prediction):\n",
                "```\n",
                "[Webhook: POST /predict]\n",
                "  â†’ [Execute Command: python scripts/predict.py --input='{{ $json.data }}']\n",
                "  â†’ [Respond to Webhook: {{ $json.prediction }}]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: predict.py script triggered by n8n webhook\n",
                "predict_script = textwrap.dedent(\"\"\"\n",
                "#!/usr/bin/env python\n",
                "# scripts/predict.py\n",
                "# Called by n8n: python predict.py --input='{\"age\":35,\"years_experience\":10,\"score\":85}'\n",
                "\n",
                "import sys\n",
                "import json\n",
                "import argparse\n",
                "import joblib\n",
                "import numpy as np\n",
                "\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument('--input', type=str, required=True)\n",
                "args = parser.parse_args()\n",
                "\n",
                "input_data = json.loads(args.input)\n",
                "model = joblib.load('models/salary_model.pkl')\n",
                "scaler = joblib.load('models/scaler.pkl')\n",
                "\n",
                "features = np.array([[input_data['age'], input_data['years_experience'], input_data['score']]])\n",
                "features_scaled = scaler.transform(features)\n",
                "prediction = model.predict(features_scaled)[0]\n",
                "\n",
                "print(json.dumps({'prediction': round(prediction, 2)}))\n",
                "\"\"\")\n",
                "print(predict_script)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4  Send Automated PDF/HTML Reports to Email / Teams / Slack\n",
                "\n",
                "The three-step pattern:\n",
                "1. Python generates a report (HTML or PDF)\n",
                "2. n8n reads the file\n",
                "3. n8n sends it via Email / Microsoft Teams / Slack node"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Generate an HTML report from Python\n",
                "def generate_html_report(df, output_path='data/exports/report.html', title='Daily Report'):\n",
                "    \"\"\"Generate a styled HTML report from a DataFrame.\"\"\"\n",
                "    now = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
                "    stats_html = df.describe().round(2).to_html(classes='table', border=0)\n",
                "    table_html = df.head(20).to_html(index=False, classes='table', border=0)\n",
                "\n",
                "    html = f\"\"\"<!DOCTYPE html>\n",
                "<html><head>\n",
                "<meta charset='utf-8'>\n",
                "<style>\n",
                "  body {{ font-family: Arial, sans-serif; margin: 32px; }}\n",
                "  h1 {{ color: #2c3e50; }}\n",
                "  .meta {{ color: gray; font-size: 0.9em; }}\n",
                "  .table {{ border-collapse: collapse; width: 100%; margin-bottom: 24px; }}\n",
                "  .table th {{ background: #2980b9; color: white; padding: 8px; text-align: left; }}\n",
                "  .table td {{ padding: 6px 8px; border-bottom: 1px solid #eee; }}\n",
                "  .table tr:hover {{ background: #f5f5f5; }}\n",
                "</style>\n",
                "</head><body>\n",
                "<h1>{title}</h1>\n",
                "<p class='meta'>Generated: {now}</p>\n",
                "<h2>Summary Statistics</h2>{stats_html}\n",
                "<h2>Data Preview (first 20 rows)</h2>{table_html}\n",
                "</body></html>\"\"\"\n",
                "\n",
                "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "    with open(output_path, 'w') as f:\n",
                "        f.write(html)\n",
                "    print(f\"âœ… Report written to: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "sample_df = pd.read_csv('data/transactions_large.csv', nrows=500) if os.path.exists('data/transactions_large.csv') else pd.DataFrame({'a': range(20), 'b': np.random.randn(20)})\n",
                "report_path = generate_html_report(sample_df, title='Daily Transaction Report')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# n8n Workflow for report delivery (conceptual)\n",
                "n8n_workflow_json = {\n",
                "    \"name\": \"Daily Report Pipeline\",\n",
                "    \"nodes\": [\n",
                "        {\"name\": \"Schedule Trigger\", \"type\": \"n8n-nodes-base.scheduleTrigger\", \"parameters\": {\"rule\": {\"interval\": [{\"field\": \"hours\", \"hoursInterval\": 24}]}}},\n",
                "        {\"name\": \"Run Python Report\", \"type\": \"n8n-nodes-base.executeCommand\", \"parameters\": {\"command\": \"python scripts/generate_report.py\"}},\n",
                "        {\"name\": \"Read Report File\", \"type\": \"n8n-nodes-base.readBinaryFile\", \"parameters\": {\"filePath\": \"data/exports/report.html\"}},\n",
                "        {\"name\": \"Send to Teams\", \"type\": \"n8n-nodes-base.microsoftTeams\", \"parameters\": {\"operation\": \"sendMessage\", \"messageType\": \"text\", \"message\": \"Daily report attached.\"}},\n",
                "        {\"name\": \"Send to Slack\", \"type\": \"n8n-nodes-base.slack\", \"parameters\": {\"operation\": \"postMessage\", \"channel\": \"#data-reports\", \"text\": \"ðŸ“Š Daily Report ready!\"}}\n",
                "    ]\n",
                "}\n",
                "print(\"n8n workflow structure (for reference):\")\n",
                "print(json.dumps(n8n_workflow_json, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Exercises\n",
                "\n",
                "1. Write a full `run_pipeline.py` script using the `log_step` decorator to chain: `load â†’ clean â†’ aggregate â†’ save_parquet`.\n",
                "2. Set up a cron job (or Windows Task) that runs your pipeline every day at 7 AM.\n",
                "3. Extend `generate_html_report` to include a bar chart image (save with matplotlib and embed as base64).\n",
                "4. Design an n8n workflow that: (a) fetches weather data every 6 hours, (b) appends it to a Parquet file, (c) sends a Slack message if temperature exceeds 35Â°C.\n",
                "5. Write a `send_teams_alert(webhook_url, message)` function using a Microsoft Teams Incoming Webhook."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}