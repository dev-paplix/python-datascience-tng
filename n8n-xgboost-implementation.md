# XGBoost Model & n8n Implementation Guide

## Best Output Source
The best and production-ready output of our modeling process is generated by the following notebook:
**`day-3/10-XGboost-model-creation.ipynb`**

This notebook accomplishes the following key goals:
1. Loads and splits the full cement quality dataset.
2. Trains an optimized XGBoost model using `GridSearchCV`.
3. Exports the final refined model as a serialized format, producing the artifact: `xgboost_cement_model.pkl`.

This `.pkl` file contains the exact mathematical structure of the trained model and is the artifact you will deploy for automation in **n8n**.

---

## How to Implement the Model inside n8n

To utilize the trained XGBoost model inside your n8n workflows for automated predictions, there are two primary approaches. The recommended approach is to wrap the model in a lightweight API (like FastAPI or Flask) and call it from n8n, as n8n is primarily designed to orchestrate APIs.

### Approach 1: The API Method (Recommended Best Practice)

By wrapping your model in an API, scaling and integration become completely decoupled from the automation tool.

1. **Create a Simple Python API (FastAPI)**
   Create a small Python script (e.g., `app.py`) that loads `xgboost_cement_model.pkl` and exposes a `/predict` endpoint.
   
   ```python
   from fastapi import FastAPI
   from pydantic import BaseModel
   import joblib
   import pandas as pd

   app = FastAPI()
   
   # Load the model saved from day-3/10-XGboost-model-creation.ipynb
   model = joblib.load('xgboost_cement_model.pkl')

   class CementFeatures(BaseModel):
       CaO: float
       SiO2: float
       Al2O3: float
       Fe2O3: float
       MgO: float
       SO3: float
       K2O: float
       Na2O: float
       P2O5: float
       TiO2: float

   @app.post("/predict")
   def predict_cement_strength(features: CementFeatures):
       # Convert input format to DataFrame
       df = pd.DataFrame([features.dict()])
       prediction = model.predict(df)
       return {"prediction": float(prediction[0])}
   ```

2. **Run Your API Server**
   Start the API server on your machine or a cloud host using Uvicorn:
   ```bash
   uvicorn app:app --host 0.0.0.0 --port 8000
   ```

3. **Call the API from n8n**
   - Add an **HTTP Request** node in your n8n workflow.
   - **Method:** `POST`
   - **URL:** `http://localhost:8000/predict` (or your deployed URL)
   - **Send Body:** On
   - **Body Parameters:** Select JSON and map your incoming workflow data (e.g., from a Webhook, Form, or Database) to the API's required fields:
     ```json
     {
       "CaO": {{$json.CaO}},
       "SiO2": {{$json.SiO2}},
       "Al2O3": {{$json.Al2O3}},
       "Fe2O3": {{$json.Fe2O3}},
       "MgO": {{$json.MgO}},
       "SO3": {{$json.SO3}},
       "K2O": {{$json.K2O}},
       "Na2O": {{$json.Na2O}},
       "P2O5": {{$json.P2O5}},
       "TiO2": {{$json.TiO2}}
     }
     ```
   - Run the node to ensure it returns the response `{"prediction": <value>}`.

### Approach 2: Using the n8n "Execute Command" Node

If n8n is hosted on the same server where you can run Python locally, you can use a CLI script instead of an active API. Note this has slightly more overhead per execution as the model needs to be loaded into memory for every prediction.

1. **Create a CLI Script (e.g., `predict.py`)**
   ```python
   import sys
   import json
   import pandas as pd
   import joblib

   # Load model
   model = joblib.load('xgboost_cement_model.pkl')
   
   # Parse JSON arguments passed from n8n
   input_data = json.loads(sys.argv[1])
   df = pd.DataFrame([input_data])
   
   # Generate prediction and output to stdout
   prediction = model.predict(df)
   print(json.dumps({"prediction": float(prediction[0])}))
   ```

2. **Configure the n8n Execute Command node**
   - Add the **Execute Command** node to your n8n workflow.
   - **Command:** `python /abs/path/to/predict.py '{{JSON.stringify($json.features)}}'` 
   - Ensure the machine running n8n has a Python environment configured with `xgboost`, `pandas`, and `scikit-learn`.
   - The stdout output of this node will just be the JSON string. You can use an n8n **Code** node to parse it via `JSON.parse` to make routing decisions later.

---

## Example n8n Automation Ideas
* **Trigger:** An engineer inputs new compound metrics into a Google Form or a row is added to a PostgreSQL database.
* **Processing:** n8n pulls that exact row, formats the values, and requests a prediction from your deployed XGBoost model.
* **Action:** 
  - If the predicted `Res 45 um` violates quality parameters, n8n sends an immediate Slack/Teams alert to the Quality Assurance manager.
  - If it passes, the data is just routed back and updated in the main production tracking database.
